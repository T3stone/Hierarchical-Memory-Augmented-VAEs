{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da17bec-1b2e-45a4-a019-00dfdd043a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.utils as vutils\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import time\n",
    "start_time = datetime.datetime.now()\n",
    "print(start_time)\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e56a47-4161-49b9-98be-6650c200df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5552c4ed-c757-45eb-b67c-fc9b15a78b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict\n",
    "model_path_hmvae = './saved_models_hmvae_fasionmnist/model_epoch_491_loss_24.2987.pt'\n",
    "state_dict = torch.load(model_path_hmvae, map_location=device)\n",
    "model_path_MEMvae = './saved_models_MEMvae_fasionmnist/model_epoch_487_loss_24.0129.pt'\n",
    "state_dict_MEMvae = torch.load(model_path_MEMvae, map_location=device)\n",
    "model_path_vae = './saved_models_vae_fasionmnist/model_epoch_489_loss_24.6418.pt'\n",
    "state_dict_vae = torch.load(model_path_vae, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6b8bc8-e6c9-42b7-93e2-48c1aa3aa7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "mnist_test = FashionMNIST(root=\"./fashiondata\", train=False, download=True)\n",
    "real_images_dir = \"./fasionmnist_real_images\"\n",
    "os.makedirs(real_images_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "for i, (img_pil, label) in enumerate(mnist_test):\n",
    "    img_pil.save(os.path.join(real_images_dir, f\"image_{i:04d}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ab315-76bf-4a11-98be-46a6b760febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HMVAE import HMVAE\n",
    "n_levels = 3\n",
    "n_regions=2\n",
    "region_slot_config= [\n",
    "    ( [50, 50, 50], [64, 128, 256] ),  \n",
    "    ( [64, 64,64],   [32,64, 128] )      \n",
    "]\n",
    "latent_dim = 20\n",
    "hidden_dim =128\n",
    "image_channels = 1  # MNIST has 1 channel\n",
    "image_size = 28  # 28x28 images\n",
    "model_hmvae = HMVAE( n_levels, n_regions, region_slot_config,latent_dim,hidden_dim,image_channels,image_size).to(device)\n",
    "model_hmvae.load_state_dict(state_dict)\n",
    "model_hmvae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabedfc4-9ad5-42a6-8d70-f25a8db501ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading 1-layers\n",
    "from MEMVAE import MEMVAE\n",
    "n_levels = 1\n",
    "num_slots_list = [50]  # Just example sizes\n",
    "slot_dim_list = [64]  # Example embedding sizes\n",
    "latent_dim = 20\n",
    "hidden_dim =128\n",
    "image_channels = 1  # MNIST has 1 channel\n",
    "image_size = 28  # 28x28 images\n",
    "model_MEMvae= MEMVAE( n_levels, num_slots_list, slot_dim_list,latent_dim,hidden_dim,image_channels,image_size).to(device)\n",
    "model_MEMvae.load_state_dict(state_dict_MEMvae)\n",
    "model_MEMvae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b3c29a-9b83-40ba-a488-4dd2135e1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from VAE import VAE\n",
    "model_vae  = VAE(latent_dim=20,hidden_dim=128,image_channels=1,image_size=28).to(device)\n",
    "model_vae.load_state_dict(state_dict_vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7778676e-a27a-41b4-a179-81bc8c3d8cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_latent(model, num_samples, save_dir, latent_dim=20):\n",
    "    model.eval()\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        \n",
    "        if model == model_vae:\n",
    "            gen_data = model.decoder(z)  \n",
    "        else:\n",
    "            gen_data, _ = model.decoder(z)  \n",
    "            \n",
    "        gen_data = gen_data.view(-1, 1, 28, 28)\n",
    "        \n",
    "       \n",
    "        for i in range(num_samples):\n",
    "            img_path = os.path.join(save_dir, f'image_{i:04d}.png')  \n",
    "            vutils.save_image(gen_data[i], img_path, normalize=False) \n",
    "\n",
    "\n",
    "generate_from_latent(model_vae, num_samples=10000, save_dir='./generated_images_vae_fasion')\n",
    "generate_from_latent(model_hmvae, num_samples=10000, save_dir='./generated_images_hmvae_fasion')\n",
    "generate_from_latent(model_MEMvae, num_samples=10000, save_dir='./generated_images_MEMVAE_fasion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480f3e02-60f8-477a-a8b9-c4a9ec39eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from torchvision.utils import make_grid, save_image\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "# Corrected model name spelling from \"fasion\" to \"fashion\"\n",
    "models = [\"vae_fashion\", \"hmvae_fashion\", \"memvae_fashion\"]\n",
    "\n",
    "output_dir = \"./generated_grids_fashion\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for model in models:\n",
    "    input_folder = f\"./generated_images_{model}\"\n",
    "    \n",
    "    # Check if input folder exists\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Warning: {input_folder} does not exist, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Get all PNG images in the folder\n",
    "    all_images = glob.glob(os.path.join(input_folder, \"*.png\"))\n",
    "    \n",
    "    # Check if we have enough images\n",
    "    if len(all_images) < 64:\n",
    "        print(f\"Warning: Insufficient images in {input_folder} (found {len(all_images)}, need 64), skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Randomly sample 64 images\n",
    "    image_paths = random.sample(all_images, 64)\n",
    "    \n",
    "    # Load and process images\n",
    "    tensor_list = []\n",
    "    transform = transforms.ToTensor()\n",
    "    for img_path in image_paths:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "        tensor_list.append(tensor)\n",
    "    \n",
    "    # Create image grid\n",
    "    batch = torch.cat(tensor_list, dim=0)\n",
    "    grid = make_grid(batch, nrow=8, padding=2, normalize=True)\n",
    "    \n",
    "    # Save grid image\n",
    "    output_path = os.path.join(output_dir, f\"{model}_grid_8x8.png\")\n",
    "    save_image(grid, output_path)\n",
    "    print(f\"{model} grid image saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517439d9-d919-4c62-bab0-53b30355365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def convert_grayscale_to_rgb(image_path):\n",
    "    img = Image.open(image_path).convert(\"L\")  \n",
    "    img_rgb = np.stack([np.array(img)] * 3, axis=-1)  \n",
    "    Image.fromarray(img_rgb).save(image_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a0bcfc4-117c-4b0a-b823-2e426e5c6706",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path in glob.glob(\"./generated_images_vae_fasion/*.png\"):\n",
    "    convert_grayscale_to_rgb(img_path)\n",
    "for img_path in glob.glob(\"./generated_images_hmvae_fasion/*.png\"):\n",
    "    convert_grayscale_to_rgb(img_path)\n",
    "for img_path in glob.glob(\"./generated_images_MEMVAE_fasion/*.png\"):\n",
    "    convert_grayscale_to_rgb(img_path)\n",
    "for img_path in glob.glob(\"./fasionmnist_real_images/*.png\"):\n",
    "    convert_grayscale_to_rgb(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca2377d-e1dd-4959-89f0-3e62d00f484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "models = [\n",
    "    \"vae_fasion\", \n",
    "    \"hmvae_fasion\", \n",
    "    \"MEMVAE_fasion\"\n",
    "]\n",
    "for model in models:\n",
    "    gen_dir = f\"./generated_images_{model}\"\n",
    "    cmd = f\"python -m pytorch_fid  {gen_dir} ./fasionmnist_real_images\"\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    print(f\"FID for {model}: {result.stdout}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae428d91-c054-44ca-b3f7-7195ae3da9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65aaf37-800b-4f13-a567-0825ce8e149d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
